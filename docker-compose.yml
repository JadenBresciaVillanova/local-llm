version: '3.8'

services:
  postgres:
    image: postgres:15-alpine
    container_name: rag_postgres
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: rag_db
    volumes:
      - ./.postgres-data:/var/lib/postgresql/data
      - ./infra/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    restart: unless-stopped

  mongo:
    image: mongo:6.0
    container_name: rag_mongo
    environment:
      MONGO_INITDB_ROOT_USERNAME: user
      MONGO_INITDB_ROOT_PASSWORD: password
    ports:
      - "27017:27017"
    volumes:
      - ./.mongo-data:/data/db
    restart: unless-stopped

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./infra/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3001:3000" # Use 3001 to avoid conflict with Next.js default port
    volumes:
      - ./.grafana-data:/var/lib/grafana
    restart: unless-stopped

# Optional: If you want to containerize Ollama instead of running it on the host.
# This requires GPU passthrough configuration.

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    # This 'deploy' section is essential for GPU access.
    # It tells Docker Compose to reserve all available NVIDIA GPUs for this container.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 'all' # Use 'all' to pass through all available GPUs
              capabilities: [gpu]
    # This 'volumes' section is key to avoiding re-downloading models.
    # It maps your local Ollama models directory into the container.
    #
    # IMPORTANT: Replace the path before the colon (:) with the actual path
    # to your Ollama models directory on your host machine.
    #
    # For Windows (like yours): C:\Users\YourUsername\.ollama
    # For Linux/WSL/MacOS: ~/.ollama
    volumes:
      - C:\Users\jb100\.ollama:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped